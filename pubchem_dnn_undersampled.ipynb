{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview ##\n",
    "\n",
    "PubChem is a site run by the NIH which hosts raw data associated with chemical experiments; here we analyze the data hosted at PubChem for assay 1030, which looks for inhibitors of the protein encoding gene ALDH1A1. You can access the page for this assay [here](https://pubchem.ncbi.nlm.nih.gov/bioassay/1030)\n",
    "\n",
    "## Results ##\n",
    "\n",
    "We use the SMILES string, a common representation for a molecule amongst chemists, to begin the featurization process. Because the length of this string varies, it is normalized in the form of a Morgan Fingerprint; these are then used to train various DNN-based classifiers in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exploratory data analysis and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem, rdMolDescriptors\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "global_random_state = 42\n",
    "\n",
    "np.random.seed(global_random_state)\n",
    "\n",
    "\n",
    "active_pct = 0.073125471\n",
    "inactive_pct = 1 - active_pct\n",
    "\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "fh = logging.FileHandler('log_dnn.txt')\n",
    "fh.setLevel(logging.INFO)\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "ch = logging.StreamHandler(sys.stdout)\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-27 11:29:52,381 - INFO - Processed index: 10000\n",
      "2017-09-27 11:29:57,808 - INFO - Processed index: 30000\n",
      "2017-09-27 11:30:00,628 - INFO - Processed index: 40000\n",
      "2017-09-27 11:30:03,485 - INFO - Processed index: 50000\n",
      "2017-09-27 11:30:06,415 - INFO - Processed index: 60000\n",
      "2017-09-27 11:30:12,158 - INFO - Processed index: 80000\n",
      "2017-09-27 11:30:14,984 - INFO - Processed index: 90000\n",
      "2017-09-27 11:30:17,902 - INFO - Processed index: 100000\n",
      "2017-09-27 11:30:20,756 - INFO - Processed index: 110000\n",
      "2017-09-27 11:30:23,727 - INFO - Processed index: 120000\n",
      "2017-09-27 11:30:26,784 - INFO - Processed index: 130000\n",
      "2017-09-27 11:30:29,899 - INFO - Processed index: 140000\n",
      "2017-09-27 11:30:32,974 - INFO - Processed index: 150000\n",
      "2017-09-27 11:30:39,052 - INFO - Processed index: 170000\n",
      "2017-09-27 11:30:50,860 - INFO - Processed index: 210000\n",
      "2017-09-27 11:30:54,383 - INFO - Processed index: 220000\n",
      "2017-09-27 11:30:54,839 - INFO - Sampling\n",
      "2017-09-27 11:30:54,938 - INFO - Compute classes statistics ...\n",
      "2017-09-27 11:30:54,964 - INFO - 2 classes detected: Counter({0: 148299, 1: 16111})\n",
      "2017-09-27 11:30:55,388 - INFO - Under-sampling performed: Counter({1: 16111, 0: 16111})\n",
      "2017-09-27 11:30:55,388 - INFO - Processed all, pickling\n"
     ]
    }
   ],
   "source": [
    "# and downloading the datatable\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "ba_df = pd.read_csv(\"AID_1030_datatable_all.csv\")\n",
    "\n",
    "# Load compound info\n",
    "cs_df = pd.read_csv(\"AID_1030_compound_smiles.csv\",sep='\\t',header=0)\n",
    "\n",
    "# Merge the two\n",
    "full_df = ba_df.merge(cs_df,on='PUBCHEM_CID')\n",
    "\n",
    "# Cleanup the compound ID column\n",
    "full_df[\"PUBCHEM_CID\"] = full_df[\"PUBCHEM_CID\"].astype(int)\n",
    "\n",
    "# Delete CID 3246048, which fails featurization\n",
    "full_df = full_df[full_df[\"PUBCHEM_CID\"] != 3246048]\n",
    "\n",
    "# Delete all inconclusive results\n",
    "# Delete CID 3246048, which fails featurization\n",
    "full_df = full_df[full_df[\"PUBCHEM_ACTIVITY_OUTCOME\"] != \"Inconclusive\"]\n",
    "\n",
    "compound_ids = list()\n",
    "smiles_list = list()\n",
    "fingerprints = list()\n",
    "activities = list()\n",
    "\n",
    "#fingerprint_df = \n",
    "\n",
    "for index, row in full_df.iterrows() :\n",
    "    cid = row[\"PUBCHEM_CID\"]\n",
    "    smiles_string = row[\"Smiles\"]\n",
    "    mol = Chem.MolFromSmiles(smiles_string)\n",
    "    is_active = row[\"PUBCHEM_ACTIVITY_OUTCOME\"] == \"Active\"\n",
    "    if mol is None:\n",
    "        logger.info(\"Molecule failed featurization\")\n",
    "        logger.info(index)\n",
    "    else: \n",
    "        fingerprint = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol,2,nBits=2048,useChirality=False,\n",
    "                                                                     useBondTypes=False,useFeatures=False)\n",
    "        \n",
    "        # From RDKit documentation\n",
    "        arr = np.zeros((1,))\n",
    "        DataStructs.ConvertToNumpyArray(fingerprint, arr)\n",
    "        fingerprint = arr\n",
    "        \n",
    "        compound_ids.append(cid)\n",
    "        smiles_list.append(smiles_string)\n",
    "        fingerprints.append(fingerprint)\n",
    "        activities.append(is_active)\n",
    "    \n",
    "    if index % 10000 == 0:\n",
    "        logger.info(\"Processed index: {0}\".format(index))\n",
    "\n",
    "fingerprints = np.array(fingerprints)\n",
    "activities = np.array(activities,dtype=int)\n",
    "        \n",
    "logger.info(\"Sampling\")\n",
    "\n",
    "rus = RandomUnderSampler(random_state=global_random_state)\n",
    "X, y = rus.fit_sample(fingerprints, activities)\n",
    "\n",
    "logger.info(\"Processed all, pickling\")\n",
    "\n",
    "#compound_ids_and_features = (compound_ids, smiles_list, fingerprints, activities)\n",
    "\n",
    "# Pickle the data to save time in the future\n",
    "with open('data.classification.undersampled.pickle', 'wb') as f:\n",
    "    pickle.dump((X,y), f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.backend.backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16110/16110 [==============================] - 1s - loss: 0.6033 - acc: 0.6732     \n",
      "Epoch 2/10\n",
      "16110/16110 [==============================] - 0s - loss: 0.4373 - acc: 0.7946     \n",
      "Epoch 3/10\n",
      "16110/16110 [==============================] - 0s - loss: 0.2361 - acc: 0.9022     \n",
      "Epoch 4/10\n",
      "16110/16110 [==============================] - 0s - loss: 0.0959 - acc: 0.9666     \n",
      "Epoch 5/10\n",
      "16110/16110 [==============================] - 0s - loss: 0.0463 - acc: 0.9857     \n",
      "Epoch 6/10\n",
      "16110/16110 [==============================] - 0s - loss: 0.0261 - acc: 0.9928     \n",
      "Epoch 7/10\n",
      "16110/16110 [==============================] - 0s - loss: 0.0132 - acc: 0.9958     \n",
      "Epoch 8/10\n",
      "16110/16110 [==============================] - 0s - loss: 0.0086 - acc: 0.9976     \n",
      "Epoch 9/10\n",
      "16110/16110 [==============================] - 0s - loss: 0.0159 - acc: 0.9959     \n",
      "Epoch 10/10\n",
      "16110/16110 [==============================] - 0s - loss: 0.0130 - acc: 0.9967     \n",
      "11000/16112 [===================>..........] - ETA: 0s2017-09-27 11:39:47,851 - INFO -              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.77      0.74      8056\n",
      "          1       0.75      0.68      0.71      8056\n",
      "\n",
      "avg / total       0.73      0.73      0.73     16112\n",
      "\n",
      "Epoch 1/10\n",
      "16112/16112 [==============================] - 1s - loss: 0.6056 - acc: 0.6697     \n",
      "Epoch 2/10\n",
      "16112/16112 [==============================] - 0s - loss: 0.4369 - acc: 0.7953     \n",
      "Epoch 3/10\n",
      "16112/16112 [==============================] - 0s - loss: 0.2317 - acc: 0.9100     \n",
      "Epoch 4/10\n",
      "16112/16112 [==============================] - 0s - loss: 0.1003 - acc: 0.9647     \n",
      "Epoch 5/10\n",
      "16112/16112 [==============================] - 0s - loss: 0.0489 - acc: 0.9840     \n",
      "Epoch 6/10\n",
      "16112/16112 [==============================] - 0s - loss: 0.0346 - acc: 0.9890     \n",
      "Epoch 7/10\n",
      "16112/16112 [==============================] - 0s - loss: 0.0230 - acc: 0.9930     \n",
      "Epoch 8/10\n",
      "16112/16112 [==============================] - 0s - loss: 0.0108 - acc: 0.9968     \n",
      "Epoch 9/10\n",
      "16112/16112 [==============================] - 0s - loss: 0.0081 - acc: 0.9977     \n",
      "Epoch 10/10\n",
      "16112/16112 [==============================] - 0s - loss: 0.0063 - acc: 0.9971     \n",
      "16000/16110 [============================>.] - ETA: 0s2017-09-27 11:39:57,337 - INFO -              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.79      0.75      8055\n",
      "          1       0.77      0.67      0.72      8055\n",
      "\n",
      "avg / total       0.74      0.73      0.73     16110\n",
      "\n",
      "2017-09-27 11:39:57,339 - INFO - Average roc_auc score of 2 folds is: 0.7302156243970666\n"
     ]
    }
   ],
   "source": [
    "# What about a deep neural network?\n",
    "# Sample code from: https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import metrics\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import pickle\n",
    "\n",
    "k_fold_splits = 2\n",
    "global_random_state = 42\n",
    "\n",
    "with open('data.classification.undersampled.pickle', 'rb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    (X, y) = pickle.load(f)\n",
    "\n",
    "#class_weights = compute_class_weight('balanced', np.unique(y), y)\n",
    "\n",
    "    \n",
    "def create_model() :\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2000, input_dim=2048, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "skf = StratifiedKFold(n_splits=k_fold_splits,shuffle=True,random_state=global_random_state)\n",
    "\n",
    "roc_auc_avg = 0\n",
    "\n",
    "for train_index, test_index in skf.split(X,y) :\n",
    "\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "    classifier = KerasClassifier(build_fn=create_model, epochs=10, batch_size=100, verbose=1)\n",
    "    classifier.fit(X_train,y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    logger.info(classification_report(y_test, y_pred))\n",
    "    # What is the AUC-ROC score?\n",
    "    auc = roc_auc_score(y_test, y_pred, average='macro', sample_weight=None)\n",
    "    roc_auc_avg = roc_auc_avg + auc\n",
    "    \n",
    "roc_auc_avg = roc_auc_avg / k_fold_splits\n",
    "logger.info(\"Average roc_auc score of {} folds is: {}\".format(k_fold_splits, roc_auc_avg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-27 11:41:51,224 - INFO - Processed index: 10000\n",
      "2017-09-27 11:41:56,663 - INFO - Processed index: 30000\n",
      "2017-09-27 11:41:59,473 - INFO - Processed index: 40000\n",
      "2017-09-27 11:42:02,307 - INFO - Processed index: 50000\n",
      "2017-09-27 11:42:05,228 - INFO - Processed index: 60000\n",
      "2017-09-27 11:42:10,961 - INFO - Processed index: 80000\n",
      "2017-09-27 11:42:13,811 - INFO - Processed index: 90000\n",
      "2017-09-27 11:42:16,753 - INFO - Processed index: 100000\n",
      "2017-09-27 11:42:19,614 - INFO - Processed index: 110000\n",
      "2017-09-27 11:42:22,614 - INFO - Processed index: 120000\n",
      "2017-09-27 11:42:25,669 - INFO - Processed index: 130000\n",
      "2017-09-27 11:42:28,722 - INFO - Processed index: 140000\n",
      "2017-09-27 11:42:31,770 - INFO - Processed index: 150000\n",
      "2017-09-27 11:42:37,820 - INFO - Processed index: 170000\n",
      "2017-09-27 11:42:50,020 - INFO - Processed index: 210000\n",
      "2017-09-27 11:42:53,539 - INFO - Processed index: 220000\n",
      "2017-09-27 11:42:54,013 - INFO - Sampling\n",
      "2017-09-27 11:42:54,116 - INFO - Compute classes statistics ...\n",
      "2017-09-27 11:42:54,140 - INFO - 2 classes detected: Counter({0: 148299, 1: 16111})\n",
      "2017-09-27 11:44:47,842 - INFO - Over-sampling performed: Counter({0: 148299, 1: 148299})\n",
      "2017-09-27 11:45:06,417 - INFO - Processed all, pickling\n"
     ]
    }
   ],
   "source": [
    "# Let's try oversampling\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ba_df = pd.read_csv(\"AID_1030_datatable_all.csv\")\n",
    "\n",
    "# Load compound info\n",
    "cs_df = pd.read_csv(\"AID_1030_compound_smiles.csv\",sep='\\t',header=0)\n",
    "\n",
    "# Merge the two\n",
    "full_df = ba_df.merge(cs_df,on='PUBCHEM_CID')\n",
    "\n",
    "# Cleanup the compound ID column\n",
    "full_df[\"PUBCHEM_CID\"] = full_df[\"PUBCHEM_CID\"].astype(int)\n",
    "\n",
    "# Delete CID 3246048, which fails featurization\n",
    "full_df = full_df[full_df[\"PUBCHEM_CID\"] != 3246048]\n",
    "\n",
    "# Delete all inconclusive results\n",
    "# Delete CID 3246048, which fails featurization\n",
    "full_df = full_df[full_df[\"PUBCHEM_ACTIVITY_OUTCOME\"] != \"Inconclusive\"]\n",
    "\n",
    "compound_ids = list()\n",
    "smiles_list = list()\n",
    "fingerprints = list()\n",
    "activities = list()\n",
    "\n",
    "#fingerprint_df = \n",
    "\n",
    "for index, row in full_df.iterrows() :\n",
    "    cid = row[\"PUBCHEM_CID\"]\n",
    "    smiles_string = row[\"Smiles\"]\n",
    "    mol = Chem.MolFromSmiles(smiles_string)\n",
    "    is_active = row[\"PUBCHEM_ACTIVITY_OUTCOME\"] == \"Active\"\n",
    "    if mol is None:\n",
    "        logger.info(\"Molecule failed featurization\")\n",
    "        logger.info(index)\n",
    "    else: \n",
    "        fingerprint = rdMolDescriptors.GetMorganFingerprintAsBitVect(mol,2,nBits=2048,useChirality=False,\n",
    "                                                                     useBondTypes=False,useFeatures=False)\n",
    "        \n",
    "        # From RDKit documentation\n",
    "        arr = np.zeros((1,))\n",
    "        DataStructs.ConvertToNumpyArray(fingerprint, arr)\n",
    "        fingerprint = arr\n",
    "        \n",
    "        compound_ids.append(cid)\n",
    "        smiles_list.append(smiles_string)\n",
    "        fingerprints.append(fingerprint)\n",
    "        activities.append(is_active)\n",
    "    \n",
    "    if index % 10000 == 0:\n",
    "        logger.info(\"Processed index: {0}\".format(index))\n",
    "\n",
    "fingerprints = np.array(fingerprints)\n",
    "activities = np.array(activities,dtype=int)\n",
    "        \n",
    "logger.info(\"Sampling\")\n",
    "\n",
    "ros = RandomOverSampler(random_state=global_random_state)\n",
    "X, y = ros.fit_sample(fingerprints, activities)\n",
    "\n",
    "logger.info(\"Processed all, pickling\")\n",
    "\n",
    "# Pickle the data to save time in the future\n",
    "with open('data.classification.oversampled.pickle', 'wb') as f:\n",
    "    pickle.dump((X,y), f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "148298/148298 [==============================] - 8s - loss: 0.3842 - acc: 0.8283     \n",
      "Epoch 2/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.1360 - acc: 0.9512     \n",
      "Epoch 3/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0625 - acc: 0.9792     \n",
      "Epoch 4/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0413 - acc: 0.9870     \n",
      "Epoch 5/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0310 - acc: 0.9907     \n",
      "Epoch 6/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0262 - acc: 0.9921     \n",
      "Epoch 7/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0209 - acc: 0.9939     \n",
      "Epoch 8/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0180 - acc: 0.9947     \n",
      "Epoch 9/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0161 - acc: 0.9951     \n",
      "Epoch 10/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0133 - acc: 0.9961     \n",
      "Epoch 11/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0126 - acc: 0.9962     \n",
      "Epoch 12/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0118 - acc: 0.9967     \n",
      "Epoch 13/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0119 - acc: 0.9966     \n",
      "Epoch 14/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0092 - acc: 0.9972     \n",
      "Epoch 15/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0099 - acc: 0.9971     \n",
      "Epoch 16/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0089 - acc: 0.9975     \n",
      "Epoch 17/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0080 - acc: 0.9978     \n",
      "Epoch 18/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0089 - acc: 0.9976     \n",
      "Epoch 19/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0088 - acc: 0.9975     \n",
      "Epoch 20/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0082 - acc: 0.9978     \n",
      "Epoch 21/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0090 - acc: 0.9979     \n",
      "Epoch 22/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0091 - acc: 0.9980     \n",
      "Epoch 23/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0077 - acc: 0.9979     \n",
      "Epoch 24/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0068 - acc: 0.9981     \n",
      "Epoch 25/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0069 - acc: 0.9981     \n",
      "Epoch 26/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0067 - acc: 0.9980     \n",
      "Epoch 27/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0058 - acc: 0.9984     \n",
      "Epoch 28/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0062 - acc: 0.9983     \n",
      "Epoch 29/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0068 - acc: 0.9981     \n",
      "Epoch 30/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0047 - acc: 0.9985     \n",
      "Epoch 31/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0051 - acc: 0.9984     \n",
      "Epoch 32/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0047 - acc: 0.9985     \n",
      "Epoch 33/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0056 - acc: 0.9983     \n",
      "Epoch 34/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0049 - acc: 0.9985     \n",
      "Epoch 35/50\n",
      "148298/148298 [==============================] - 8s - loss: 0.0052 - acc: 0.9983     \n",
      "Epoch 36/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0046 - acc: 0.9985     \n",
      "Epoch 37/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0043 - acc: 0.9986     \n",
      "Epoch 38/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0055 - acc: 0.9983     \n",
      "Epoch 39/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0062 - acc: 0.9984     \n",
      "Epoch 40/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0070 - acc: 0.9985     \n",
      "Epoch 41/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0070 - acc: 0.9984     \n",
      "Epoch 42/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0052 - acc: 0.9985     \n",
      "Epoch 43/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0041 - acc: 0.9986     \n",
      "Epoch 44/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0034 - acc: 0.9988     \n",
      "Epoch 45/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0034 - acc: 0.9987     \n",
      "Epoch 46/50\n",
      "148298/148298 [==============================] - 8s - loss: 0.0047 - acc: 0.9985     \n",
      "Epoch 47/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0053 - acc: 0.9985     \n",
      "Epoch 48/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0042 - acc: 0.9987     \n",
      "Epoch 49/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0060 - acc: 0.9985     \n",
      "Epoch 50/50\n",
      "148298/148298 [==============================] - 7s - loss: 0.0041 - acc: 0.9986     \n",
      "143200/148300 [===========================>..] - ETA: 0s2017-09-27 18:49:03,087 - INFO -              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.95      0.97     74150\n",
      "          1       0.95      0.99      0.97     74150\n",
      "\n",
      "avg / total       0.97      0.97      0.97    148300\n",
      "\n",
      "Epoch 1/50\n",
      "148300/148300 [==============================] - 8s - loss: 0.3827 - acc: 0.8293     \n",
      "Epoch 2/50\n",
      "148300/148300 [==============================] - 8s - loss: 0.1365 - acc: 0.9517     \n",
      "Epoch 3/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0634 - acc: 0.9786     \n",
      "Epoch 4/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0393 - acc: 0.9870     \n",
      "Epoch 5/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0331 - acc: 0.9897     \n",
      "Epoch 6/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0237 - acc: 0.9923     \n",
      "Epoch 7/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0217 - acc: 0.9932     \n",
      "Epoch 8/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0185 - acc: 0.9947     \n",
      "Epoch 9/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0160 - acc: 0.9954     \n",
      "Epoch 10/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0159 - acc: 0.9954     \n",
      "Epoch 11/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0129 - acc: 0.9964     \n",
      "Epoch 12/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0128 - acc: 0.9962     \n",
      "Epoch 13/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0115 - acc: 0.9966     \n",
      "Epoch 14/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0109 - acc: 0.9968     \n",
      "Epoch 15/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0103 - acc: 0.9972     \n",
      "Epoch 16/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0100 - acc: 0.9972     \n",
      "Epoch 17/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0088 - acc: 0.9977     \n",
      "Epoch 18/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0078 - acc: 0.9979     \n",
      "Epoch 19/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0080 - acc: 0.9977     \n",
      "Epoch 20/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0074 - acc: 0.9978     \n",
      "Epoch 21/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0065 - acc: 0.9981     \n",
      "Epoch 22/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0069 - acc: 0.9980     \n",
      "Epoch 23/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0063 - acc: 0.9980     \n",
      "Epoch 24/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0072 - acc: 0.9982     \n",
      "Epoch 25/50\n",
      "148300/148300 [==============================] - 8s - loss: 0.0064 - acc: 0.9982     \n",
      "Epoch 26/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0079 - acc: 0.9981     \n",
      "Epoch 27/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0071 - acc: 0.9982     \n",
      "Epoch 28/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0057 - acc: 0.9984     \n",
      "Epoch 29/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0055 - acc: 0.9985     \n",
      "Epoch 30/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0066 - acc: 0.9981     \n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148300/148300 [==============================] - 7s - loss: 0.0053 - acc: 0.9984     \n",
      "Epoch 32/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0049 - acc: 0.9986     \n",
      "Epoch 33/50\n",
      "148300/148300 [==============================] - 8s - loss: 0.0060 - acc: 0.9984     \n",
      "Epoch 34/50\n",
      "148300/148300 [==============================] - 8s - loss: 0.0055 - acc: 0.9982     \n",
      "Epoch 35/50\n",
      "148300/148300 [==============================] - 8s - loss: 0.0044 - acc: 0.9986     \n",
      "Epoch 36/50\n",
      "148300/148300 [==============================] - 8s - loss: 0.0044 - acc: 0.9986     \n",
      "Epoch 37/50\n",
      "148300/148300 [==============================] - 8s - loss: 0.0057 - acc: 0.9985     \n",
      "Epoch 38/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0054 - acc: 0.9984     \n",
      "Epoch 39/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0044 - acc: 0.9986     \n",
      "Epoch 40/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0046 - acc: 0.9986     \n",
      "Epoch 41/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0054 - acc: 0.9985     \n",
      "Epoch 42/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0042 - acc: 0.9988     \n",
      "Epoch 43/50\n",
      "148300/148300 [==============================] - 8s - loss: 0.0055 - acc: 0.9985     \n",
      "Epoch 44/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0047 - acc: 0.9986     \n",
      "Epoch 45/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0046 - acc: 0.9987     \n",
      "Epoch 46/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0067 - acc: 0.9985     \n",
      "Epoch 47/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0055 - acc: 0.9985     \n",
      "Epoch 48/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0043 - acc: 0.9987     \n",
      "Epoch 49/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0047 - acc: 0.9988     \n",
      "Epoch 50/50\n",
      "148300/148300 [==============================] - 7s - loss: 0.0039 - acc: 0.9989     \n",
      "148298/148298 [==============================] - 1s     \n",
      "2017-09-27 18:55:42,827 - INFO -              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.94      0.97     74149\n",
      "          1       0.95      0.99      0.97     74149\n",
      "\n",
      "avg / total       0.97      0.97      0.97    148298\n",
      "\n",
      "2017-09-27 18:55:42,864 - INFO - Average roc_auc score of 2 folds is: 0.9696828655050751\n"
     ]
    }
   ],
   "source": [
    "# What about a deep neural network?\n",
    "# Sample code from: https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import metrics\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import pickle\n",
    "\n",
    "k_fold_splits = 2\n",
    "global_random_state = 42\n",
    "\n",
    "with open('data.classification.oversampled.pickle', 'rb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    (X, y) = pickle.load(f)\n",
    "\n",
    "#class_weights = compute_class_weight('balanced', np.unique(y), y)\n",
    "\n",
    "print(\"Number of samples is: {}\".format(X))\n",
    "    \n",
    "def create_model() :\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2000, input_dim=2048, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "skf = StratifiedKFold(n_splits=k_fold_splits,shuffle=True,random_state=global_random_state)\n",
    "\n",
    "roc_auc_avg = 0\n",
    "\n",
    "for train_index, test_index in skf.split(X,y) :\n",
    "\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    print(\"Number of training samples is: {}\".format(X_train))\n",
    "    print(\"Number of test samples is: {}\".format(X_test))\n",
    "\n",
    "    classifier = KerasClassifier(build_fn=create_model, epochs=10, batch_size=100, verbose=1)\n",
    "    classifier.fit(X_train,y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    logger.info(classification_report(y_test, y_pred))\n",
    "    # What is the AUC-ROC score?\n",
    "    auc = roc_auc_score(y_test, y_pred, average='macro', sample_weight=None)\n",
    "    roc_auc_avg = roc_auc_avg + auc\n",
    "    \n",
    "roc_auc_avg = roc_auc_avg / k_fold_splits\n",
    "logger.info(\"Average roc_auc score of {} folds is: {}\".format(k_fold_splits, roc_auc_avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples is: [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Number of training samples is: [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Number of test samples is: [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "2017-09-27 20:10:04,674 - INFO -              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.50      0.50     74150\n",
      "          1       0.50      0.50      0.50     74150\n",
      "\n",
      "avg / total       0.50      0.50      0.50    148300\n",
      "\n",
      "Number of training samples is: [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "Number of test samples is: [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "2017-09-27 20:10:05,675 - INFO -              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.50      0.50     74149\n",
      "          1       0.50      0.50      0.50     74149\n",
      "\n",
      "avg / total       0.50      0.50      0.50    148298\n",
      "\n",
      "2017-09-27 20:10:05,714 - INFO - Average roc_auc score of 2 folds is: 0.49989212345461725\n"
     ]
    }
   ],
   "source": [
    "# What is the performance of a dummy classifier on the training set?\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import pickle\n",
    "\n",
    "k_fold_splits = 2\n",
    "global_random_state = 42\n",
    "\n",
    "with open('data.classification.oversampled.pickle', 'rb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    (X, y) = pickle.load(f)\n",
    "\n",
    "print(\"Number of samples is: {}\".format(len(X)))\n",
    "    \n",
    "skf = StratifiedKFold(n_splits=k_fold_splits,shuffle=True,random_state=global_random_state)\n",
    "\n",
    "roc_auc_avg = 0\n",
    "\n",
    "for train_index, test_index in skf.split(X,y) :\n",
    "\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    print(\"Number of training samples is: {}\".format(len(X_train)))\n",
    "    print(\"Number of test samples is: {}\".format(len(X_test)))\n",
    "\n",
    "    classifier = DummyClassifier(random_state=rglobal_random_statew)\n",
    "    classifier.fit(X_train,y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    logger.info(classification_report(y_test, y_pred))\n",
    "    # What is the AUC-ROC score?\n",
    "    auc = roc_auc_score(y_test, y_pred, average='macro', sample_weight=None)\n",
    "    roc_auc_avg = roc_auc_avg + auc\n",
    "    \n",
    "roc_auc_avg = roc_auc_avg / k_fold_splits\n",
    "logger.info(\"Average roc_auc score of {} folds is: {}\".format(k_fold_splits, roc_auc_avg))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
